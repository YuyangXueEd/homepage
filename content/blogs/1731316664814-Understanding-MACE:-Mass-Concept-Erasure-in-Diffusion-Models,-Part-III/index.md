---
title: "Understanding MACE: Mass Concept Erasure in Diffusion Models, Part III"
date: 2024-11-10
draft: false
series: ["MACE"]
series_order: 3
description: "The training process, the dataset, and the configuration"
tags: ["machine unlearning", "concept erasure"]
---

{{< alert "heart" >}}
This is a series of blogs that detailed recorded the exploration of the paper [MACE: Mass Concept Erasure in Diffusion Models](https://arxiv.org/abs/2403.06135). The thumbnail image is generated by [Stable Diffusion 3.5 Large](https://huggingface.co/spaces/stabilityai/stable-diffusion-3.5-large) using the prompt "Machine Unlearning".
{{< /alert >}}

{{<katex>}}

Here is the paper link and its github repository:

> Paper: [MACE: Mass Concept Erasure in Diffusion Models](https://arxiv.org/abs/2403.06135)
> {{< github repo="Shilin-LU/MACE">}}

## Training Process

In this blog, we will first discuss the training process of the MACE implementation. It used the same paradigm as the `diffusers` LoRA fine-tuning process. We skip the standard initialisation of `accelerator` and start by loading the pipeline:

```python {class="my-class" id="my-codeblock" lineNos=inline tabWidth=2}
# import tokenizer and text encoder
tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder="tokenizer", 
                                            revision=args.revision, use_fast=False)
text_encoder_cls = CLIPTextModel(args.pretrained_model_name_or_path, args.revision)

# Load scheduler and models
noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision)
unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="unet", revision=args.revision)
```

Now that we have the models, we now set these model to freeze in case we tune them by accident:

```python {class="my-class" id="my-codeblock" lineNos=inline tabWidth=2}
unet.to(accelerator.device, dtype=weight_dtype)
vae.requires_grad_(False)
unet.requires_grad_(False)
if not args.train_text_encoder:
    text_encoder.requires_grad_(False)
```

### `MACEDataset`

In MACE framework, in order to gather all the information for the fine-tuning process, they constructed a dataset called `MACEDataset` that is used for pre-process the prompt and augmentation. First, let's take a quick look of the configuration file with some necessary terms from a celebrity config file:

```yaml
multi_concept:
    - [ 
        [melania-trump, object], 
      ]
use_pooler: true  
train_batch_size: 1
mapping_concept: 
- 'a woman'
augment: true
aug_length: 30
prompt_len: 30
use_gpt: false
prior_preservation_cache_path: ./cache/cache_coco.pt
domain_preservation_cache_path: ./cache/cache_cele.pt
input_data_dir: ./data/1cele
```

The definition of the dataset is located in `dataset.py` file and looks like this:

```python {class="my-class" id="my-codeblock" lineNos=inline tabWidth=2}
class MACEDataset(Dataset):
"""
A dataset to prepare the instance and class images with the prompts for fine-tuning the model.
It pre-processes the images and the tokenizes prompts.
"""

def __init__(
    self,
    tokenizer,
    size=512,
    center_crop=False,
    use_pooler=False,
    multi_concept=None,
    mapping=None,
    augment=True,
    batch_size=None,
    with_prior_preservation=False,
    preserve_info=None,
    num_class_images=None,
    train_seperate=False,
    aug_length=50,
    prompt_len=250,
    input_data_path=None,
    use_gpt=False,
):  
    """
    toeknizer: the pretrained tokenizer,
    size: the image size
    center_crop: whether using center_crop in transform
    use_pooler: the special token "<|endoftext|>"
    multi_concept: the corresponding unlearning prompts
    mapping: the corresponding anchoring prompts
    augment: whether augment the prompt using LLM
    batch_size: the training batchsize
    with_prior_preservation: whether to add preservation information
    preserve_info: 
    num_class_images: how many classes of images to preserve
    train_seperate:
    aug_length: the number of augmented prompts
    prompt_len: the length of prompt
    input_data_path: the input data directory
    use_gpt: whether to use LLM to generate synonymous prompt
    """
    
    # will skip the initialisation part
    
    for concept_idx, (data, mapping_concept) in enumerate(zip(multi_concept, mapping)):
        c, t = data # c = 'melania-trump', t = object, mapping_concept='a woman'
        
        # the image data generated with the unlearning prompt 'melania-trump' 
        if input_data_path is not None:
            # change 'melania-trump' to 'melania trump' and get the images
            p = Path(os.path.join(input_data_path, c.replace("-", " ")))
            if not p.exists():
                raise ValueError(f"Instance {p} images root doesn't exists.")
            
            # get the segmented mask of the image if it is belongs to `object` category
            if t == "object":
                p_mask = Path(os.path.join(input_data_path, c.replace("-", " ")).replace(f'{c.replace("-", " ")}', f'{c.replace("-", " ")} mask'))
                if not p_mask.exists():
                    raise ValueError(f"Instance {p_mask} images root doesn't exists.")
        else:
            raise ValueError(f"Input data path is not provided.")    
        
        image_paths = list(p.iterdir())
        single_concept_images_path = []
        # get all images path
        single_concept_images_path += image_paths
        self.all_concept_image_path.append(single_concept_images_path)
        
        if t == "object":
            mask_paths = list(p_mask.iterdir())
            single_concept_masks_path = []
            # get all mask path
            single_concept_masks_path += mask_paths
            self.all_concept_mask_path.append(single_concept_masks_path)
                 
        # the unlearning prompt
        erased_concept = c.replace("-", " ")
        
        # use LLM to generate prompts
        if use_gpt:
            class_prompt_collection, mapping_prompt_collection = text_augmentation(erased_concept, mapping_concept, t, num_text_augmentations=self.aug_length)
            self.instance_prompt.append(class_prompt_collection)
            self.target_prompt.append(mapping_prompt_collection)
        else: 
            # `self.aug_length` number of prompt from templates
            sampled_indices = random.sample(range(0, prompt_len), self.aug_length)
            self.instance_prompt.append(prompt_augmentation(erased_concept, augment=augment, sampled_indices=sampled_indices, concept_type=t))
            self.target_prompt.append(prompt_augmentation(mapping_concept, augment=augment, sampled_indices=sampled_indices, concept_type=t))
            
        self.num_instance_images += len(single_concept_images_path)
        
        # prepare the e^f and e^g entries
        entry = {"old": self.instance_prompt[concept_idx], "new": self.target_prompt[concept_idx]}
        self.dict_for_close_form.append(entry)
        
    # if have piror_preservation, need also prepare the prompt and image generation with the preservation prompt
    if with_prior_preservation:
        class_data_root = Path(preserve_info['preserve_data_dir'])
        if os.path.isdir(class_data_root):
            class_images_path = list(class_data_root.iterdir())
            class_prompt = [preserve_info["preserve_prompt"] for _ in range(len(class_images_path))]
        else:
            with open(class_data_root, "r") as f:
                class_images_path = f.read().splitlines()
            with open(preserve_info["preserve_prompt"], "r") as f:
                class_prompt = f.read().splitlines()

        class_img_path = [(x, y) for (x, y) in zip(class_images_path, class_prompt)]
        self.class_images_path.extend(class_img_path[:num_class_images])
    
    # transforms for the images
    self.image_transforms = transforms.Compose(
        [
            # transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),
            transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ]
    )
    
    self._concept_num = len(self.instance_prompt)
    self.num_class_images = len(self.class_images_path)
    self._length = max(self.num_instance_images // self._concept_num, self.num_class_images)
    
    # skip __len__
    
    def __getitem__(self, index):
        example = {}
        
        if not self.train_seperate:
            if self.batch_counter % self.batch_size == 0:
                self.concept_number = random.randint(0, self._concept_num - 1)
            self.batch_counter += 1
        
        # get the image
        instance_image = Image.open(self.all_concept_image_path[self.concept_number][index % self._length])
        
        if len(self.all_concept_mask_path) == 0:
            # artistic style erasure
            binary_tensor = None
        else:
            # object/celebrity erasure, get the corresponding mask image
            instance_mask = Image.open(self.all_concept_mask_path[self.concept_number][index % self._length])
            instance_mask = instance_mask.convert('L')
            trans = transforms.ToTensor()
            binary_tensor = trans(instance_mask)
        
        # get a random prompt from the list
        prompt_number = random.randint(0, len(self.instance_prompt[self.concept_number]) - 1)
        instance_prompt, target_tokens = self.instance_prompt[self.concept_number][prompt_number]
        
        if not instance_image.mode == "RGB":
            instance_image = instance_image.convert("RGB")
        example["instance_prompt"] = instance_prompt
        example["instance_images"] = self.image_transforms(instance_image)
        example["instance_masks"] = binary_tensor
        
        # tokenize prompts
        example["instance_prompt_ids"] = self.tokenizer(
            instance_prompt,
            truncation=True,
            padding="max_length",
            max_length=self.tokenizer.model_max_length,
            return_tensors="pt",
        ).input_ids
        prompt_ids = self.tokenizer(
            instance_prompt,
            truncation=True,
            padding="max_length",
            max_length=self.tokenizer.model_max_length
        ).input_ids
        # get anchor token
        concept_ids = self.tokenizer(
            target_tokens,
            add_special_tokens=False
        ).input_ids             

        pooler_token_id = self.tokenizer(
            "<|endoftext|>",
            add_special_tokens=False
        ).input_ids[0]

        concept_positions = [0] * self.tokenizer.model_max_length
        # Loop to Find the Positions of Concept Tokens and Pooler Token
        for i, tok_id in enumerate(prompt_ids):
            if tok_id == concept_ids[0] and prompt_ids[i:i + len(concept_ids)] == concept_ids:
                concept_positions[i:i + len(concept_ids)] = [1]*len(concept_ids)
            if self.use_pooler and tok_id == pooler_token_id:
                concept_positions[i] = 1
        # Storing concept_positions
        example["concept_positions"] = torch.tensor(concept_positions)[None]               

        if self.with_prior_preservation:
            class_image, class_prompt = self.class_images_path[index % self.num_class_images]
            class_image = Image.open(class_image)
            if not class_image.mode == "RGB":
                class_image = class_image.convert("RGB")
            example["preserve_images"] = self.image_transforms(class_image)
            example["preserve_prompt_ids"] = self.tokenizer(
                class_prompt,
                padding="max_length",
                truncation=True,
                max_length=self.tokenizer.model_max_length,
                return_tensors="pt",
            ).input_ids
            
        return example
```

The instance `example` of the `MACEDataset` contains several useful elements:

```python {class="my-class" id="my-codeblock" lineNos=inline tabWidth=2}
"""
example["instance_prompt"]: the unlearning instance prompt
example["instance_images"]: the corresponding unlearning instance image
example["instance_masks"]: the corresponding unlearning instance image mask
example["instance_prompt_ids"]: the unlearning tokenized id
example["concept_positions"]: where the core concept located in the prompt
example["preserve_images"]: the preservation image
example["preserve_prompt_ids"]: the preservation tokenized id
"""
```

### Training Process (continued)

Let's get back to the training process, after instantiating the `train_dataset` and `train_loader`, and recalculating the number of train epochs, we come to the first stage, the introduced `closed-form refinement`.

```python {class="my-class" id="my-codeblock" lineNos=inline tabWidth=2}
# stage 1: closed-form refinement
# get the correspoding layers and modules
projection_matrices, ca_layers, og_matrices = get_ca_layers(unet, with_to_k=True)

# to save memory
CFR_dict = {}
max_concept_num = args.max_memory # the maximum number of concept that can be processed at once
if len(train_dataset.dict_for_close_form) > max_concept_num:
    
    for layer_num in tqdm(range(len(projection_matrices))):
        CFR_dict[f'{layer_num}_for_mat1'] = None
        CFR_dict[f'{layer_num}_for_mat2'] = None
        
    for i in tqdm(range(0, len(train_dataset.dict_for_close_form), max_concept_num)):
        contexts_sub, valuess_sub = prepare_k_v(text_encoder, projection_matrices, ca_layers, og_matrices, 
                                                train_dataset.dict_for_close_form[i:i+5], tokenizer, all_words=args.all_words)
        closed_form_refinement(projection_matrices, contexts_sub, valuess_sub, cache_dict=CFR_dict, cache_mode=True)
        
        del contexts_sub, valuess_sub
        gc.collect()
        torch.cuda.empty_cache()
        
else:
    for layer_num in tqdm(range(len(projection_matrices))):
        CFR_dict[f'{layer_num}_for_mat1'] = .0
        CFR_dict[f'{layer_num}_for_mat2'] = .0
    # prepare for the closed-form refinement
    contexts, valuess = prepare_k_v(text_encoder, projection_matrices, ca_layers, og_matrices, 
                                    train_dataset.dict_for_close_form, tokenizer, all_words=args.all_words)

del ca_layers, og_matrices

# Load cached prior knowledge for preserving, e^p
if args.prior_preservation_cache_path:
    prior_preservation_cache_dict = torch.load(args.prior_preservation_cache_path, map_location=projection_matrices[0].weight.device)
else:
    prior_preservation_cache_dict = {}
    for layer_num in tqdm(range(len(projection_matrices))):
        prior_preservation_cache_dict[f'{layer_num}_for_mat1'] = .0
        prior_preservation_cache_dict[f'{layer_num}_for_mat2'] = .0
        
# Load cached domain knowledge for preserving, lambda_3, e^p
if args.domain_preservation_cache_path:
    domain_preservation_cache_dict = torch.load(args.domain_preservation_cache_path, map_location=projection_matrices[0].weight.device)
else:
    domain_preservation_cache_dict = {}
    for layer_num in tqdm(range(len(projection_matrices))):
        domain_preservation_cache_dict[f'{layer_num}_for_mat1'] = .0
        domain_preservation_cache_dict[f'{layer_num}_for_mat2'] = .0

# integrate the prior knowledge, domain knowledge and closed-form refinement
cache_dict = {}
for key in CFR_dict:
    cache_dict[key] = args.train_preserve_scale * (prior_preservation_cache_dict[key] \
                    + args.preserve_weight * domain_preservation_cache_dict[key]) \
                    + CFR_dict[key]

# closed-form refinement
projection_matrices, _, _ = get_ca_layers(unet, with_to_k=True)

if len(train_dataset.dict_for_close_form) > max_concept_num:
    closed_form_refinement(projection_matrices, lamb=args.lamb, preserve_scale=1, cache_dict=cache_dict)
else:
    # ge3t the final weights
    closed_form_refinement(projection_matrices, contexts, valuess, lamb=args.lamb, 
                           preserve_scale=args.train_preserve_scale, cache_dict=cache_dict)

del contexts, valuess, cache_dict
gc.collect()
torch.cuda.empty_cache()
```

## Mutli-LoRA Training

The closed-form refinement focuses on the removal of co-existing words, either generated from LLMs or from the template. The next step is to erase the target concept itself. The author proposed that the attention maps corresponding to the tokens of the concept should display high activation values in certain regions.



