---
title: "Understanding MACE: Mass Concept Erasure in Diffusion Models, Part II "
date: 2024-11-09
draft: false
series: ["MACE"]
series_order: 2
description: "Closed-form refinement, LoRA and Fusion"
tags: ["machine unlearning", "concept erasure"]
---

{{< alert "heart" >}}
This is a series of blogs that detailed recorded the exploration of the paper [MACE: Mass Concept Erasure in Diffusion Models](https://arxiv.org/abs/2403.06135). The thumbnail image is generated by [Stable Diffusion 3.5 Large](https://huggingface.co/spaces/stabilityai/stable-diffusion-3.5-large) using the prompt "Machine Unlearning".
{{< /alert >}}

{{<katex>}}

Here is the paper link and its github repository:

> Paper: [MACE: Mass Concept Erasure in Diffusion Models](https://arxiv.org/abs/2403.06135)
> {{< github repo="Shilin-LU/MACE">}}

## Closed-Form Cross-Attention Refinement

Lu et al. [^p1] suggest a closed-form cross-attention refinement to encourage the model to refrain from embedding residual information of the target phrase into other words. They looked into the cross-attention layer, where the text embedding of a token encapsulates information from other tokens. This results in its `Key` and `Value` vectors.

In machine unlearning, a standard way of forgetting a concept is to find an anchor concept and make the target concept move to the anchor. In MACE, when altering the projection matrix \\(W_k\\), they modify it such that the `Keys` of the words that coexist with the target phrase in the prompt are mapped to the `Keys` of those same words in another prompt, where the target phrase is replaced with either its super-category or a generic concept. The illustration here is to show how closed-form cross-attention refinement can help remove residual.

![closed-form refinement](closed-form_refinement.png)





[^p1]: Lu, Shilin, et al. "Mace: Mass concept erasure in diffusion models." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.