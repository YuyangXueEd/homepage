---
title: "Publications"
date: 2024-05-05
layout: "simple"
---

## Conference 

{{< timeline >}}
<b>2024</b>
{{< timelineItem icon="paper" header="✨Erase to Enhance: Data-Efficient Machine Unlearning in MRI Reconstruction" badge="MIDL" subheader="Feb, 2024" >}}

<img src="E2E.png" width="250px" style="float:left;margin-right:10px"> 

<b>Yuyang Xue</b>, Jingshuai Liu, Steven McDonagh, Sotirios A Tsaftaris
<br>
<b>Abstract</b>: Our study reveals that combin- ing training data can lead to hallucinations and reduced image quality in the reconstructed data. We use unlearning to remove hallucinations as a proxy exemplar of undesired data removal. Indeed, we show that machine unlearning is possible without full retraining. Furthermore, our observations indicate that maintaining high performance is feasible even when using only a subset of retain data.
<br>

<a href="https://openreview.net/pdf?id=FmCscsj7Ey">[Paper]</a>

<a href="https://github.com/vios-s/ReconUnlearning">[Code]</a>
{{< /timelineItem >}}

{{< timelineItem icon="paper" header="✨Inference Stage Denoising for Undersampled MRI Reconstruction" badge="ISBI" subheader="Feb, 2024" >}}

<img src="isdfumr.png" width="250" style="float:left;margin-right:10px"> 

<b>Yuyang Xue</b>, Chen Qin, Sotirios A Tsaftaris
<br>
<b>Abstract</b>: In this work, by employing a conditional hyperparameter network, we eliminate the need of augmentation, yet maintain robust performance under various levels of Gaussian noise. We demonstrate that our model withstands various input noise levels while producing high-definition reconstructions during the test stage. Moreover, we present a hyperparameter sampling strategy that accelerates the convergence of training. 
<br>
<br>
<a href="https://arxiv.org/abs/2402.08692v1">[Paper]</a>

<a href="https://github.com/vios-s/Inference_Denoising_MRI_Recon">[Code]</a>
{{< /timelineItem >}}

<b>2023</b>

{{< /timeline >}}

## Journal

## 